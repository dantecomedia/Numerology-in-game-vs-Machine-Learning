{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Dateset_2_6_19.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>day_path</th>\n",
       "      <th>away_lp_day</th>\n",
       "      <th>away_lp</th>\n",
       "      <th>home_lp_day</th>\n",
       "      <th>home_lp</th>\n",
       "      <th>home_win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.129800e+04</td>\n",
       "      <td>41298.000000</td>\n",
       "      <td>41298.000000</td>\n",
       "      <td>41298.000000</td>\n",
       "      <td>41298.000000</td>\n",
       "      <td>41298.000000</td>\n",
       "      <td>41298.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.008068e+07</td>\n",
       "      <td>7.152453</td>\n",
       "      <td>5.778827</td>\n",
       "      <td>8.094629</td>\n",
       "      <td>5.760497</td>\n",
       "      <td>8.122912</td>\n",
       "      <td>0.540002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.898686e+04</td>\n",
       "      <td>4.843864</td>\n",
       "      <td>3.051637</td>\n",
       "      <td>7.660322</td>\n",
       "      <td>3.055793</td>\n",
       "      <td>7.699747</td>\n",
       "      <td>0.498403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000033e+07</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.004052e+07</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.008063e+07</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.012082e+07</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.016100e+07</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date      day_path   away_lp_day       away_lp   home_lp_day  \\\n",
       "count  4.129800e+04  41298.000000  41298.000000  41298.000000  41298.000000   \n",
       "mean   2.008068e+07      7.152453      5.778827      8.094629      5.760497   \n",
       "std    4.898686e+04      4.843864      3.051637      7.660322      3.055793   \n",
       "min    2.000033e+07      1.000000      1.000000      1.000000      1.000000   \n",
       "25%    2.004052e+07      4.000000      3.000000      4.000000      3.000000   \n",
       "50%    2.008063e+07      7.000000      6.000000      7.000000      6.000000   \n",
       "75%    2.012082e+07      9.000000      8.000000      9.000000      8.000000   \n",
       "max    2.016100e+07     22.000000     11.000000     33.000000     11.000000   \n",
       "\n",
       "            home_lp      home_win  \n",
       "count  41298.000000  41298.000000  \n",
       "mean       8.122912      0.540002  \n",
       "std        7.699747      0.498403  \n",
       "min        1.000000      0.000000  \n",
       "25%        4.000000      0.000000  \n",
       "50%        7.000000      1.000000  \n",
       "75%        9.000000      1.000000  \n",
       "max       33.000000      1.000000  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>day_path</th>\n",
       "      <th>day_zodiac_sign</th>\n",
       "      <th>away_name</th>\n",
       "      <th>home_name</th>\n",
       "      <th>away_starting_pitcher_name</th>\n",
       "      <th>away_dob</th>\n",
       "      <th>away_lp_day</th>\n",
       "      <th>away_lp</th>\n",
       "      <th>away_zodiac</th>\n",
       "      <th>home_starting_pitcher_name</th>\n",
       "      <th>home_dob</th>\n",
       "      <th>home_lp_day</th>\n",
       "      <th>home_lp</th>\n",
       "      <th>home_zodiac</th>\n",
       "      <th>home_win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000329</td>\n",
       "      <td>7</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>CHN</td>\n",
       "      <td>NYN</td>\n",
       "      <td>Jon Lieber</td>\n",
       "      <td>4/2/1970</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Mike Hampton</td>\n",
       "      <td>9/9/1972</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Rat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000330</td>\n",
       "      <td>8</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>NYN</td>\n",
       "      <td>CHN</td>\n",
       "      <td>Rick Reed</td>\n",
       "      <td>8/16/1964</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>Kyle Farnsworth</td>\n",
       "      <td>4/14/1976</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20000403</td>\n",
       "      <td>9</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>COL</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Pedro Astacio</td>\n",
       "      <td>11/28/1968</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Monkey</td>\n",
       "      <td>Greg Maddux</td>\n",
       "      <td>4/14/1966</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Horse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20000403</td>\n",
       "      <td>9</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>MIL</td>\n",
       "      <td>CIN</td>\n",
       "      <td>Steve Woodard</td>\n",
       "      <td>5/15/1975</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>Hare</td>\n",
       "      <td>Pete Harnisch</td>\n",
       "      <td>9/23/1966</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>Horse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20000403</td>\n",
       "      <td>9</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>SFN</td>\n",
       "      <td>FLO</td>\n",
       "      <td>Livan Hernandez</td>\n",
       "      <td>2/20/1975</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>Hare</td>\n",
       "      <td>Alex Fernandez</td>\n",
       "      <td>8/13/1969</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Rooster</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  day_path day_zodiac_sign away_name home_name  \\\n",
       "0  20000329         7          Dragon       CHN       NYN   \n",
       "1  20000330         8          Dragon       NYN       CHN   \n",
       "2  20000403         9          Dragon       COL       ATL   \n",
       "3  20000403         9          Dragon       MIL       CIN   \n",
       "4  20000403         9          Dragon       SFN       FLO   \n",
       "\n",
       "  away_starting_pitcher_name    away_dob  away_lp_day  away_lp away_zodiac  \\\n",
       "0                 Jon Lieber    4/2/1970           11        5         Dog   \n",
       "1                  Rick Reed   8/16/1964            7        8      Dragon   \n",
       "2              Pedro Astacio  11/28/1968            1        9      Monkey   \n",
       "3              Steve Woodard   5/15/1975            6       33        Hare   \n",
       "4            Livan Hernandez   2/20/1975           11        8        Hare   \n",
       "\n",
       "  home_starting_pitcher_name   home_dob  home_lp_day  home_lp home_zodiac  \\\n",
       "0               Mike Hampton   9/9/1972            9        1         Rat   \n",
       "1            Kyle Farnsworth  4/14/1976            5        5      Dragon   \n",
       "2                Greg Maddux  4/14/1966            5        4       Horse   \n",
       "3              Pete Harnisch  9/23/1966            5        9       Horse   \n",
       "4             Alex Fernandez  8/13/1969            4        1     Rooster   \n",
       "\n",
       "   home_win  \n",
       "0         0  \n",
       "1         0  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.sum of         date  day_path  day_zodiac_sign  away_name  home_name  \\\n",
       "0      False     False            False      False      False   \n",
       "1      False     False            False      False      False   \n",
       "2      False     False            False      False      False   \n",
       "3      False     False            False      False      False   \n",
       "4      False     False            False      False      False   \n",
       "5      False     False            False      False      False   \n",
       "6      False     False            False      False      False   \n",
       "7      False     False            False      False      False   \n",
       "8      False     False            False      False      False   \n",
       "9      False     False            False      False      False   \n",
       "10     False     False            False      False      False   \n",
       "11     False     False            False      False      False   \n",
       "12     False     False            False      False      False   \n",
       "13     False     False            False      False      False   \n",
       "14     False     False            False      False      False   \n",
       "15     False     False            False      False      False   \n",
       "16     False     False            False      False      False   \n",
       "17     False     False            False      False      False   \n",
       "18     False     False            False      False      False   \n",
       "19     False     False            False      False      False   \n",
       "20     False     False            False      False      False   \n",
       "21     False     False            False      False      False   \n",
       "22     False     False            False      False      False   \n",
       "23     False     False            False      False      False   \n",
       "24     False     False            False      False      False   \n",
       "25     False     False            False      False      False   \n",
       "26     False     False            False      False      False   \n",
       "27     False     False            False      False      False   \n",
       "28     False     False            False      False      False   \n",
       "29     False     False            False      False      False   \n",
       "...      ...       ...              ...        ...        ...   \n",
       "41268  False     False            False      False      False   \n",
       "41269  False     False            False      False      False   \n",
       "41270  False     False            False      False      False   \n",
       "41271  False     False            False      False      False   \n",
       "41272  False     False            False      False      False   \n",
       "41273  False     False            False      False      False   \n",
       "41274  False     False            False      False      False   \n",
       "41275  False     False            False      False      False   \n",
       "41276  False     False            False      False      False   \n",
       "41277  False     False            False      False      False   \n",
       "41278  False     False            False      False      False   \n",
       "41279  False     False            False      False      False   \n",
       "41280  False     False            False      False      False   \n",
       "41281  False     False            False      False      False   \n",
       "41282  False     False            False      False      False   \n",
       "41283  False     False            False      False      False   \n",
       "41284  False     False            False      False      False   \n",
       "41285  False     False            False      False      False   \n",
       "41286  False     False            False      False      False   \n",
       "41287  False     False            False      False      False   \n",
       "41288  False     False            False      False      False   \n",
       "41289  False     False            False      False      False   \n",
       "41290  False     False            False      False      False   \n",
       "41291  False     False            False      False      False   \n",
       "41292  False     False            False      False      False   \n",
       "41293  False     False            False      False      False   \n",
       "41294  False     False            False      False      False   \n",
       "41295  False     False            False      False      False   \n",
       "41296  False     False            False      False      False   \n",
       "41297  False     False            False      False      False   \n",
       "\n",
       "       away_starting_pitcher_name  away_dob  away_lp_day  away_lp  \\\n",
       "0                           False     False        False    False   \n",
       "1                           False     False        False    False   \n",
       "2                           False     False        False    False   \n",
       "3                           False     False        False    False   \n",
       "4                           False     False        False    False   \n",
       "5                           False     False        False    False   \n",
       "6                           False     False        False    False   \n",
       "7                           False     False        False    False   \n",
       "8                           False     False        False    False   \n",
       "9                           False     False        False    False   \n",
       "10                          False     False        False    False   \n",
       "11                          False     False        False    False   \n",
       "12                          False     False        False    False   \n",
       "13                          False     False        False    False   \n",
       "14                          False     False        False    False   \n",
       "15                          False     False        False    False   \n",
       "16                          False     False        False    False   \n",
       "17                          False     False        False    False   \n",
       "18                          False     False        False    False   \n",
       "19                          False     False        False    False   \n",
       "20                          False     False        False    False   \n",
       "21                          False     False        False    False   \n",
       "22                          False     False        False    False   \n",
       "23                          False     False        False    False   \n",
       "24                          False     False        False    False   \n",
       "25                          False     False        False    False   \n",
       "26                          False     False        False    False   \n",
       "27                          False     False        False    False   \n",
       "28                          False     False        False    False   \n",
       "29                          False     False        False    False   \n",
       "...                           ...       ...          ...      ...   \n",
       "41268                       False     False        False    False   \n",
       "41269                       False     False        False    False   \n",
       "41270                       False     False        False    False   \n",
       "41271                       False     False        False    False   \n",
       "41272                       False     False        False    False   \n",
       "41273                       False     False        False    False   \n",
       "41274                       False     False        False    False   \n",
       "41275                       False     False        False    False   \n",
       "41276                       False     False        False    False   \n",
       "41277                       False     False        False    False   \n",
       "41278                       False     False        False    False   \n",
       "41279                       False     False        False    False   \n",
       "41280                       False     False        False    False   \n",
       "41281                       False     False        False    False   \n",
       "41282                       False     False        False    False   \n",
       "41283                       False     False        False    False   \n",
       "41284                       False     False        False    False   \n",
       "41285                       False     False        False    False   \n",
       "41286                       False     False        False    False   \n",
       "41287                       False     False        False    False   \n",
       "41288                       False     False        False    False   \n",
       "41289                       False     False        False    False   \n",
       "41290                       False     False        False    False   \n",
       "41291                       False     False        False    False   \n",
       "41292                       False     False        False    False   \n",
       "41293                       False     False        False    False   \n",
       "41294                       False     False        False    False   \n",
       "41295                       False     False        False    False   \n",
       "41296                       False     False        False    False   \n",
       "41297                       False     False        False    False   \n",
       "\n",
       "       away_zodiac  home_starting_pitcher_name  home_dob  home_lp_day  \\\n",
       "0            False                       False     False        False   \n",
       "1            False                       False     False        False   \n",
       "2            False                       False     False        False   \n",
       "3            False                       False     False        False   \n",
       "4            False                       False     False        False   \n",
       "5            False                       False     False        False   \n",
       "6            False                       False     False        False   \n",
       "7            False                       False     False        False   \n",
       "8            False                       False     False        False   \n",
       "9            False                       False     False        False   \n",
       "10           False                       False     False        False   \n",
       "11           False                       False     False        False   \n",
       "12           False                       False     False        False   \n",
       "13           False                       False     False        False   \n",
       "14           False                       False     False        False   \n",
       "15           False                       False     False        False   \n",
       "16           False                       False     False        False   \n",
       "17           False                       False     False        False   \n",
       "18           False                       False     False        False   \n",
       "19           False                       False     False        False   \n",
       "20           False                       False     False        False   \n",
       "21           False                       False     False        False   \n",
       "22           False                       False     False        False   \n",
       "23           False                       False     False        False   \n",
       "24           False                       False     False        False   \n",
       "25           False                       False     False        False   \n",
       "26           False                       False     False        False   \n",
       "27           False                       False     False        False   \n",
       "28           False                       False     False        False   \n",
       "29           False                       False     False        False   \n",
       "...            ...                         ...       ...          ...   \n",
       "41268        False                       False     False        False   \n",
       "41269        False                       False     False        False   \n",
       "41270        False                       False     False        False   \n",
       "41271        False                       False     False        False   \n",
       "41272        False                       False     False        False   \n",
       "41273        False                       False     False        False   \n",
       "41274        False                       False     False        False   \n",
       "41275        False                       False     False        False   \n",
       "41276        False                       False     False        False   \n",
       "41277        False                       False     False        False   \n",
       "41278        False                       False     False        False   \n",
       "41279        False                       False     False        False   \n",
       "41280        False                       False     False        False   \n",
       "41281        False                       False     False        False   \n",
       "41282        False                       False     False        False   \n",
       "41283        False                       False     False        False   \n",
       "41284        False                       False     False        False   \n",
       "41285        False                       False     False        False   \n",
       "41286        False                       False     False        False   \n",
       "41287        False                       False     False        False   \n",
       "41288        False                       False     False        False   \n",
       "41289        False                       False     False        False   \n",
       "41290        False                       False     False        False   \n",
       "41291        False                       False     False        False   \n",
       "41292        False                       False     False        False   \n",
       "41293        False                       False     False        False   \n",
       "41294        False                       False     False        False   \n",
       "41295        False                       False     False        False   \n",
       "41296        False                       False     False        False   \n",
       "41297        False                       False     False        False   \n",
       "\n",
       "       home_lp  home_zodiac  home_win  \n",
       "0        False        False     False  \n",
       "1        False        False     False  \n",
       "2        False        False     False  \n",
       "3        False        False     False  \n",
       "4        False        False     False  \n",
       "5        False        False     False  \n",
       "6        False        False     False  \n",
       "7        False        False     False  \n",
       "8        False        False     False  \n",
       "9        False        False     False  \n",
       "10       False        False     False  \n",
       "11       False        False     False  \n",
       "12       False        False     False  \n",
       "13       False        False     False  \n",
       "14       False        False     False  \n",
       "15       False        False     False  \n",
       "16       False        False     False  \n",
       "17       False        False     False  \n",
       "18       False        False     False  \n",
       "19       False        False     False  \n",
       "20       False        False     False  \n",
       "21       False        False     False  \n",
       "22       False        False     False  \n",
       "23       False        False     False  \n",
       "24       False        False     False  \n",
       "25       False        False     False  \n",
       "26       False        False     False  \n",
       "27       False        False     False  \n",
       "28       False        False     False  \n",
       "29       False        False     False  \n",
       "...        ...          ...       ...  \n",
       "41268    False        False     False  \n",
       "41269    False        False     False  \n",
       "41270    False        False     False  \n",
       "41271    False        False     False  \n",
       "41272    False        False     False  \n",
       "41273    False        False     False  \n",
       "41274    False        False     False  \n",
       "41275    False        False     False  \n",
       "41276    False        False     False  \n",
       "41277    False        False     False  \n",
       "41278    False        False     False  \n",
       "41279    False        False     False  \n",
       "41280    False        False     False  \n",
       "41281    False        False     False  \n",
       "41282    False        False     False  \n",
       "41283    False        False     False  \n",
       "41284    False        False     False  \n",
       "41285    False        False     False  \n",
       "41286    False        False     False  \n",
       "41287    False        False     False  \n",
       "41288    False        False     False  \n",
       "41289    False        False     False  \n",
       "41290    False        False     False  \n",
       "41291    False        False     False  \n",
       "41292    False        False     False  \n",
       "41293    False        False     False  \n",
       "41294    False        False     False  \n",
       "41295    False        False     False  \n",
       "41296    False        False     False  \n",
       "41297    False        False     False  \n",
       "\n",
       "[41298 rows x 16 columns]>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1316"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.away_starting_pitcher_name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.home_starting_pitcher_name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NYN', 'CHN', 'ATL', 'CIN', 'FLO', 'MON', 'SLN', 'ANA', 'BAL',\n",
       "       'MIN', 'OAK', 'TEX', 'TOR', 'ARI', 'PIT', 'SEA', 'HOU', 'KCA',\n",
       "       'TBA', 'COL', 'MIL', 'PHI', 'SDN', 'SFN', 'BOS', 'DET', 'NYA',\n",
       "       'LAN', 'CHA', 'CLE', 'WAS', 'MIA'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.home_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Dragon', 'Snake', 'Horse', 'sheep', 'Monkey', 'Rooster', 'Dog',\n",
       "       'Pig', 'Rat', 'Ox', 'Tiger', 'Hare'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.day_zodiac_sign.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name=[]\n",
    "for col in data.columns:\n",
    "    column_name.append(col) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'day_path',\n",
       " 'day_zodiac_sign',\n",
       " 'away_name',\n",
       " 'home_name',\n",
       " 'away_starting_pitcher_name',\n",
       " 'away_dob',\n",
       " 'away_lp_day',\n",
       " 'away_lp',\n",
       " 'away_zodiac',\n",
       " 'home_starting_pitcher_name',\n",
       " 'home_dob',\n",
       " 'home_lp_day',\n",
       " 'home_lp',\n",
       " 'home_zodiac',\n",
       " 'home_win']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.loc[:,['day_path','day_zodiac_sign','away_name','home_name','away_lp_day','away_lp','away_zodiac','home_lp_day','home_lp','home_zodiac']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_path</th>\n",
       "      <th>day_zodiac_sign</th>\n",
       "      <th>away_name</th>\n",
       "      <th>home_name</th>\n",
       "      <th>away_lp_day</th>\n",
       "      <th>away_lp</th>\n",
       "      <th>away_zodiac</th>\n",
       "      <th>home_lp_day</th>\n",
       "      <th>home_lp</th>\n",
       "      <th>home_zodiac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>CHN</td>\n",
       "      <td>NYN</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>Dog</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Rat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>NYN</td>\n",
       "      <td>CHN</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Dragon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>COL</td>\n",
       "      <td>ATL</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Monkey</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Horse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>MIL</td>\n",
       "      <td>CIN</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>Hare</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>Horse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>SFN</td>\n",
       "      <td>FLO</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>Hare</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Rooster</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day_path day_zodiac_sign away_name home_name  away_lp_day  away_lp  \\\n",
       "0         7          Dragon       CHN       NYN           11        5   \n",
       "1         8          Dragon       NYN       CHN            7        8   \n",
       "2         9          Dragon       COL       ATL            1        9   \n",
       "3         9          Dragon       MIL       CIN            6       33   \n",
       "4         9          Dragon       SFN       FLO           11        8   \n",
       "\n",
       "  away_zodiac  home_lp_day  home_lp home_zodiac  \n",
       "0         Dog            9        1         Rat  \n",
       "1      Dragon            5        5      Dragon  \n",
       "2      Monkey            5        4       Horse  \n",
       "3        Hare            5        9       Horse  \n",
       "4        Hare            4        1     Rooster  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_path</th>\n",
       "      <th>away_lp_day</th>\n",
       "      <th>away_lp</th>\n",
       "      <th>home_lp_day</th>\n",
       "      <th>home_lp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>41298.000000</td>\n",
       "      <td>41298.000000</td>\n",
       "      <td>41298.000000</td>\n",
       "      <td>41298.000000</td>\n",
       "      <td>41298.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.152453</td>\n",
       "      <td>5.778827</td>\n",
       "      <td>8.094629</td>\n",
       "      <td>5.760497</td>\n",
       "      <td>8.122912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.843864</td>\n",
       "      <td>3.051637</td>\n",
       "      <td>7.660322</td>\n",
       "      <td>3.055793</td>\n",
       "      <td>7.699747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           day_path   away_lp_day       away_lp   home_lp_day       home_lp\n",
       "count  41298.000000  41298.000000  41298.000000  41298.000000  41298.000000\n",
       "mean       7.152453      5.778827      8.094629      5.760497      8.122912\n",
       "std        4.843864      3.051637      7.660322      3.055793      7.699747\n",
       "min        1.000000      1.000000      1.000000      1.000000      1.000000\n",
       "25%        4.000000      3.000000      4.000000      3.000000      4.000000\n",
       "50%        7.000000      6.000000      7.000000      6.000000      7.000000\n",
       "75%        9.000000      8.000000      9.000000      8.000000      9.000000\n",
       "max       22.000000     11.000000     33.000000     11.000000     33.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['day_zodiac_sign']=lb.fit_transform(X['day_zodiac_sign'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['away_name']=lb.fit_transform(X['away_name'])\n",
    "X['home_name']=lb.fit_transform(X['home_name'])\n",
    "X['away_zodiac']=lb.fit_transform(X['away_zodiac'])\n",
    "X['home_zodiac']=lb.fit_transform(X['home_zodiac'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_path</th>\n",
       "      <th>day_zodiac_sign</th>\n",
       "      <th>away_name</th>\n",
       "      <th>home_name</th>\n",
       "      <th>away_lp_day</th>\n",
       "      <th>away_lp</th>\n",
       "      <th>away_zodiac</th>\n",
       "      <th>home_lp_day</th>\n",
       "      <th>home_lp</th>\n",
       "      <th>home_zodiac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>41298.000000</td>\n",
       "      <td>41298.000000</td>\n",
       "      <td>41298.000000</td>\n",
       "      <td>41298.000000</td>\n",
       "      <td>41298.000000</td>\n",
       "      <td>41298.000000</td>\n",
       "      <td>41298.000000</td>\n",
       "      <td>41298.000000</td>\n",
       "      <td>41298.000000</td>\n",
       "      <td>41298.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.152453</td>\n",
       "      <td>5.530001</td>\n",
       "      <td>15.343043</td>\n",
       "      <td>15.345247</td>\n",
       "      <td>5.778827</td>\n",
       "      <td>8.094629</td>\n",
       "      <td>5.556322</td>\n",
       "      <td>5.760497</td>\n",
       "      <td>8.122912</td>\n",
       "      <td>5.567340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.843864</td>\n",
       "      <td>3.550068</td>\n",
       "      <td>9.390701</td>\n",
       "      <td>9.392839</td>\n",
       "      <td>3.051637</td>\n",
       "      <td>7.660322</td>\n",
       "      <td>3.445470</td>\n",
       "      <td>3.055793</td>\n",
       "      <td>7.699747</td>\n",
       "      <td>3.448202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           day_path  day_zodiac_sign     away_name     home_name  \\\n",
       "count  41298.000000     41298.000000  41298.000000  41298.000000   \n",
       "mean       7.152453         5.530001     15.343043     15.345247   \n",
       "std        4.843864         3.550068      9.390701      9.392839   \n",
       "min        1.000000         0.000000      0.000000      0.000000   \n",
       "25%        4.000000         3.000000      7.000000      7.000000   \n",
       "50%        7.000000         5.000000     15.000000     16.000000   \n",
       "75%        9.000000         9.000000     24.000000     24.000000   \n",
       "max       22.000000        11.000000     31.000000     31.000000   \n",
       "\n",
       "        away_lp_day       away_lp   away_zodiac   home_lp_day       home_lp  \\\n",
       "count  41298.000000  41298.000000  41298.000000  41298.000000  41298.000000   \n",
       "mean       5.778827      8.094629      5.556322      5.760497      8.122912   \n",
       "std        3.051637      7.660322      3.445470      3.055793      7.699747   \n",
       "min        1.000000      1.000000      0.000000      1.000000      1.000000   \n",
       "25%        3.000000      4.000000      3.000000      3.000000      4.000000   \n",
       "50%        6.000000      7.000000      6.000000      6.000000      7.000000   \n",
       "75%        8.000000      9.000000      9.000000      8.000000      9.000000   \n",
       "max       11.000000     33.000000     11.000000     11.000000     33.000000   \n",
       "\n",
       "        home_zodiac  \n",
       "count  41298.000000  \n",
       "mean       5.567340  \n",
       "std        3.448202  \n",
       "min        0.000000  \n",
       "25%        3.000000  \n",
       "50%        6.000000  \n",
       "75%        9.000000  \n",
       "max       11.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fbab8513f60>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask=np.zeros_like(corr,dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)]=True\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values,mask=mask,cmap='summer_r',vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>day_path</th>\n",
       "      <th>away_lp_day</th>\n",
       "      <th>away_lp</th>\n",
       "      <th>home_lp_day</th>\n",
       "      <th>home_lp</th>\n",
       "      <th>home_win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.060787</td>\n",
       "      <td>-0.019940</td>\n",
       "      <td>0.051028</td>\n",
       "      <td>-0.016576</td>\n",
       "      <td>0.047709</td>\n",
       "      <td>-0.002557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day_path</th>\n",
       "      <td>0.060787</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001636</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>-0.003498</td>\n",
       "      <td>-0.009632</td>\n",
       "      <td>-0.000139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_lp_day</th>\n",
       "      <td>-0.019940</td>\n",
       "      <td>-0.001636</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006197</td>\n",
       "      <td>-0.010272</td>\n",
       "      <td>-0.001877</td>\n",
       "      <td>0.002665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_lp</th>\n",
       "      <td>0.051028</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>-0.006197</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001058</td>\n",
       "      <td>-0.006336</td>\n",
       "      <td>-0.003072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_lp_day</th>\n",
       "      <td>-0.016576</td>\n",
       "      <td>-0.003498</td>\n",
       "      <td>-0.010272</td>\n",
       "      <td>-0.001058</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005660</td>\n",
       "      <td>0.002372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_lp</th>\n",
       "      <td>0.047709</td>\n",
       "      <td>-0.009632</td>\n",
       "      <td>-0.001877</td>\n",
       "      <td>-0.006336</td>\n",
       "      <td>-0.005660</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_win</th>\n",
       "      <td>-0.002557</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>-0.003072</td>\n",
       "      <td>0.002372</td>\n",
       "      <td>-0.009459</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  day_path  away_lp_day   away_lp  home_lp_day   home_lp  \\\n",
       "date         1.000000  0.060787    -0.019940  0.051028    -0.016576  0.047709   \n",
       "day_path     0.060787  1.000000    -0.001636  0.000835    -0.003498 -0.009632   \n",
       "away_lp_day -0.019940 -0.001636     1.000000 -0.006197    -0.010272 -0.001877   \n",
       "away_lp      0.051028  0.000835    -0.006197  1.000000    -0.001058 -0.006336   \n",
       "home_lp_day -0.016576 -0.003498    -0.010272 -0.001058     1.000000 -0.005660   \n",
       "home_lp      0.047709 -0.009632    -0.001877 -0.006336    -0.005660  1.000000   \n",
       "home_win    -0.002557 -0.000139     0.002665 -0.003072     0.002372 -0.009459   \n",
       "\n",
       "             home_win  \n",
       "date        -0.002557  \n",
       "day_path    -0.000139  \n",
       "away_lp_day  0.002665  \n",
       "away_lp     -0.003072  \n",
       "home_lp_day  0.002372  \n",
       "home_lp     -0.009459  \n",
       "home_win     1.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=1, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=1, random_state=None,max_features='auto')\n",
    "clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5418886198547216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "score=accuracy_score(Y_pred,Y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=1, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_estimators': [100, 500], 'max_depth': [1, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = { \n",
    "    'n_estimators': [100, 500],\n",
    "    'max_depth':[1,30]}\n",
    "    #max_features': ['auto', 'sqrt', 'log2']\n",
    "#}\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=clf, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mean_fit_time',\n",
       " 'mean_score_time',\n",
       " 'mean_test_score',\n",
       " 'mean_train_score',\n",
       " 'param_max_depth',\n",
       " 'param_n_estimators',\n",
       " 'params',\n",
       " 'rank_test_score',\n",
       " 'split0_test_score',\n",
       " 'split0_train_score',\n",
       " 'split1_test_score',\n",
       " 'split1_train_score',\n",
       " 'split2_test_score',\n",
       " 'split2_train_score',\n",
       " 'split3_test_score',\n",
       " 'split3_train_score',\n",
       " 'split4_test_score',\n",
       " 'split4_train_score',\n",
       " 'std_fit_time',\n",
       " 'std_score_time',\n",
       " 'std_test_score',\n",
       " 'std_train_score']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(CV_rfc.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 1, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "print(CV_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn  import tree\n",
    "model=tree.DecisionTreeClassifier(max_depth=10)\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5273607748184019\n"
     ]
    }
   ],
   "source": [
    "Y_pred=model.predict(X_test)\n",
    "score=accuracy_score(Y_pred,Y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = { \n",
    "    'max_depth':[1,200]}\n",
    "\n",
    "CV_tree = GridSearchCV(estimator=model, param_grid=param_grid, cv= 5)\n",
    "CV_tree.fit(X_train, Y_train)\n",
    "print(CV_tree.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5052461662631155\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model=KNeighborsClassifier(n_neighbors=30)\n",
    "model.fit(X_train,Y_train)\n",
    "Y_pre=model.predict(X_test)\n",
    "acc=accuracy_score(Y_test,Y_pre)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 1}\n"
     ]
    }
   ],
   "source": [
    "param_grid = { \n",
    "    'n_neighbors':[1,30]}\n",
    "\n",
    "CV_knn = GridSearchCV(estimator=model, param_grid=param_grid, cv= 5)\n",
    "CV_knn.fit(X_train, Y_train)\n",
    "print(CV_tree.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a1ebb085558c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "model=svm.SVC(kernel='linear', C=1,gamma=1)\n",
    "model.fit(X_train,Y_train)\n",
    "Y_pred=model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5418886198547216\n"
     ]
    }
   ],
   "source": [
    "accuracy =accuracy_score(Y_test,Y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5415657788539144\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "model=svm.SVC(kernel='rbf', C=10,gamma=1)\n",
    "model.fit(X_train,Y_train)\n",
    "Y_pred=model.predict(X_test)\n",
    "accuracy =accuracy_score(Y_test,Y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5418886198547216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg=LogisticRegression(C=1 , solver='newton-cg')\n",
    "logreg.fit(X_train,Y_train)\n",
    "Y_pred=logreg.predict(X_test)\n",
    "accuracy =accuracy_score(Y_test,Y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41298, 10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rosa-mystica/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=10, units=128, kernel_initializer=\"normal\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/home/rosa-mystica/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=256, kernel_initializer=\"normal\")`\n",
      "  \n",
      "/home/rosa-mystica/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=64, kernel_initializer=\"normal\")`\n",
      "  if __name__ == '__main__':\n",
      "/home/rosa-mystica/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"linear\", units=1, kernel_initializer=\"normal\")`\n",
      "  del sys.path[0]\n",
      "/home/rosa-mystica/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 128)               1408      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 50,945\n",
      "Trainable params: 50,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "28908/28908 [==============================] - 4s 143us/step - loss: 0.7098 - acc: 0.5091\n",
      "Epoch 2/100\n",
      "28908/28908 [==============================] - 4s 132us/step - loss: 0.6942 - acc: 0.5212\n",
      "Epoch 3/100\n",
      "28908/28908 [==============================] - 4s 131us/step - loss: 0.6916 - acc: 0.5313\n",
      "Epoch 4/100\n",
      "28908/28908 [==============================] - 4s 131us/step - loss: 0.6911 - acc: 0.5348\n",
      "Epoch 5/100\n",
      "28908/28908 [==============================] - 4s 134us/step - loss: 0.6910 - acc: 0.5375\n",
      "Epoch 6/100\n",
      "28908/28908 [==============================] - 4s 131us/step - loss: 0.6908 - acc: 0.5377\n",
      "Epoch 7/100\n",
      "28908/28908 [==============================] - 4s 134us/step - loss: 0.6906 - acc: 0.5384\n",
      "Epoch 8/100\n",
      "28908/28908 [==============================] - 4s 131us/step - loss: 0.6903 - acc: 0.5381\n",
      "Epoch 9/100\n",
      "28908/28908 [==============================] - 4s 131us/step - loss: 0.6900 - acc: 0.5393\n",
      "Epoch 10/100\n",
      "28908/28908 [==============================] - 4s 134us/step - loss: 0.6899 - acc: 0.5401\n",
      "Epoch 11/100\n",
      "28908/28908 [==============================] - 4s 133us/step - loss: 0.6898 - acc: 0.5406\n",
      "Epoch 12/100\n",
      "28908/28908 [==============================] - 4s 133us/step - loss: 0.6897 - acc: 0.5394\n",
      "Epoch 13/100\n",
      "28908/28908 [==============================] - 4s 132us/step - loss: 0.6895 - acc: 0.5409\n",
      "Epoch 14/100\n",
      "28908/28908 [==============================] - 4s 132us/step - loss: 0.6898 - acc: 0.5400\n",
      "Epoch 15/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6892 - acc: 0.5372\n",
      "Epoch 16/100\n",
      "28908/28908 [==============================] - 4s 133us/step - loss: 0.6892 - acc: 0.5410\n",
      "Epoch 17/100\n",
      "28908/28908 [==============================] - 4s 133us/step - loss: 0.6891 - acc: 0.5422\n",
      "Epoch 18/100\n",
      "28908/28908 [==============================] - 4s 135us/step - loss: 0.6890 - acc: 0.5418\n",
      "Epoch 19/100\n",
      "28908/28908 [==============================] - 4s 135us/step - loss: 0.6896 - acc: 0.5426\n",
      "Epoch 20/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6886 - acc: 0.5410\n",
      "Epoch 21/100\n",
      "28908/28908 [==============================] - 4s 135us/step - loss: 0.6897 - acc: 0.5428\n",
      "Epoch 22/100\n",
      "28908/28908 [==============================] - 4s 134us/step - loss: 0.6886 - acc: 0.5435\n",
      "Epoch 23/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6899 - acc: 0.5435\n",
      "Epoch 24/100\n",
      "28908/28908 [==============================] - 4s 135us/step - loss: 0.6885 - acc: 0.5419\n",
      "Epoch 25/100\n",
      "28908/28908 [==============================] - 4s 135us/step - loss: 0.6895 - acc: 0.5421\n",
      "Epoch 26/100\n",
      "28908/28908 [==============================] - 4s 135us/step - loss: 0.6883 - acc: 0.5438\n",
      "Epoch 27/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6896 - acc: 0.5419\n",
      "Epoch 28/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6880 - acc: 0.5462\n",
      "Epoch 29/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6890 - acc: 0.5442\n",
      "Epoch 30/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6936 - acc: 0.5453\n",
      "Epoch 31/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6913 - acc: 0.5442\n",
      "Epoch 32/100\n",
      "28908/28908 [==============================] - 4s 135us/step - loss: 0.6879 - acc: 0.5433\n",
      "Epoch 33/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6881 - acc: 0.5452\n",
      "Epoch 34/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6883 - acc: 0.5449\n",
      "Epoch 35/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6878 - acc: 0.5448\n",
      "Epoch 36/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6882 - acc: 0.5452\n",
      "Epoch 37/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6888 - acc: 0.5444\n",
      "Epoch 38/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6886 - acc: 0.5449\n",
      "Epoch 39/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6887 - acc: 0.5460\n",
      "Epoch 40/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6902 - acc: 0.5432\n",
      "Epoch 41/100\n",
      "28908/28908 [==============================] - 4s 135us/step - loss: 0.6889 - acc: 0.5450\n",
      "Epoch 42/100\n",
      "28908/28908 [==============================] - 4s 144us/step - loss: 0.6874 - acc: 0.5466\n",
      "Epoch 43/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6890 - acc: 0.5456\n",
      "Epoch 44/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6878 - acc: 0.5466\n",
      "Epoch 45/100\n",
      "28908/28908 [==============================] - 4s 132us/step - loss: 0.6871 - acc: 0.5446\n",
      "Epoch 46/100\n",
      "28908/28908 [==============================] - 4s 140us/step - loss: 0.6882 - acc: 0.5451\n",
      "Epoch 47/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6902 - acc: 0.5447\n",
      "Epoch 48/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6893 - acc: 0.5458\n",
      "Epoch 49/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6886 - acc: 0.5462\n",
      "Epoch 50/100\n",
      "28908/28908 [==============================] - 4s 141us/step - loss: 0.6875 - acc: 0.5453\n",
      "Epoch 51/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6885 - acc: 0.5453\n",
      "Epoch 52/100\n",
      "28908/28908 [==============================] - 4s 141us/step - loss: 0.6943 - acc: 0.5461\n",
      "Epoch 53/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6964 - acc: 0.5464\n",
      "Epoch 54/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6903 - acc: 0.5443\n",
      "Epoch 55/100\n",
      "28908/28908 [==============================] - 4s 140us/step - loss: 0.6891 - acc: 0.5478\n",
      "Epoch 56/100\n",
      "28908/28908 [==============================] - 4s 140us/step - loss: 0.6917 - acc: 0.5463\n",
      "Epoch 57/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6868 - acc: 0.5465\n",
      "Epoch 58/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6880 - acc: 0.5461\n",
      "Epoch 59/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6888 - acc: 0.5445\n",
      "Epoch 60/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6893 - acc: 0.5454\n",
      "Epoch 61/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6928 - acc: 0.5439\n",
      "Epoch 62/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6898 - acc: 0.5434\n",
      "Epoch 63/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6905 - acc: 0.5391\n",
      "Epoch 64/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6883 - acc: 0.5432\n",
      "Epoch 65/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6911 - acc: 0.5441\n",
      "Epoch 66/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6918 - acc: 0.5439\n",
      "Epoch 67/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6893 - acc: 0.5445\n",
      "Epoch 68/100\n",
      "28908/28908 [==============================] - 4s 143us/step - loss: 0.6880 - acc: 0.5446\n",
      "Epoch 69/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6931 - acc: 0.5439\n",
      "Epoch 70/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6904 - acc: 0.5468\n",
      "Epoch 71/100\n",
      "28908/28908 [==============================] - 4s 145us/step - loss: 0.6904 - acc: 0.5439\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28908/28908 [==============================] - 4s 130us/step - loss: 0.6881 - acc: 0.5434\n",
      "Epoch 73/100\n",
      "28908/28908 [==============================] - 4s 130us/step - loss: 0.6882 - acc: 0.5475\n",
      "Epoch 74/100\n",
      "28908/28908 [==============================] - 4s 129us/step - loss: 0.6899 - acc: 0.5465\n",
      "Epoch 75/100\n",
      "28908/28908 [==============================] - 4s 132us/step - loss: 0.6896 - acc: 0.5452\n",
      "Epoch 76/100\n",
      "28908/28908 [==============================] - 4s 131us/step - loss: 0.6974 - acc: 0.5428\n",
      "Epoch 77/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6915 - acc: 0.5446\n",
      "Epoch 78/100\n",
      "28908/28908 [==============================] - 4s 132us/step - loss: 0.6899 - acc: 0.5436\n",
      "Epoch 79/100\n",
      "28908/28908 [==============================] - 4s 132us/step - loss: 0.6956 - acc: 0.5458\n",
      "Epoch 80/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6904 - acc: 0.5429\n",
      "Epoch 81/100\n",
      "28908/28908 [==============================] - 4s 140us/step - loss: 0.6886 - acc: 0.5473\n",
      "Epoch 82/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6914 - acc: 0.5447\n",
      "Epoch 83/100\n",
      "28908/28908 [==============================] - 4s 135us/step - loss: 0.6889 - acc: 0.5455\n",
      "Epoch 84/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6906 - acc: 0.5450\n",
      "Epoch 85/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6895 - acc: 0.5460\n",
      "Epoch 86/100\n",
      "28908/28908 [==============================] - 4s 135us/step - loss: 0.6916 - acc: 0.5424\n",
      "Epoch 87/100\n",
      "28908/28908 [==============================] - 4s 135us/step - loss: 0.6955 - acc: 0.5437\n",
      "Epoch 88/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6881 - acc: 0.5461\n",
      "Epoch 89/100\n",
      "28908/28908 [==============================] - 4s 134us/step - loss: 0.6921 - acc: 0.5453\n",
      "Epoch 90/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6882 - acc: 0.5464\n",
      "Epoch 91/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6882 - acc: 0.5476\n",
      "Epoch 92/100\n",
      "28908/28908 [==============================] - 4s 143us/step - loss: 0.6894 - acc: 0.5441\n",
      "Epoch 93/100\n",
      "28908/28908 [==============================] - 4s 135us/step - loss: 0.7024 - acc: 0.5447\n",
      "Epoch 94/100\n",
      "28908/28908 [==============================] - 4s 135us/step - loss: 0.6890 - acc: 0.5440\n",
      "Epoch 95/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.7021 - acc: 0.5439\n",
      "Epoch 96/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.7013 - acc: 0.5422\n",
      "Epoch 97/100\n",
      "28908/28908 [==============================] - 4s 143us/step - loss: 0.6924 - acc: 0.5464\n",
      "Epoch 98/100\n",
      "28908/28908 [==============================] - 4s 140us/step - loss: 0.6930 - acc: 0.5454\n",
      "Epoch 99/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6921 - acc: 0.5441\n",
      "Epoch 100/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6892 - acc: 0.5484\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "classifier=Sequential()\n",
    "classifier.add(Dense(output_dim = 128, init = 'normal', activation = 'relu', input_dim = 10))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 256, init = 'normal', activation = 'relu'))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(output_dim = 64, init = 'normal', activation = 'relu'))\n",
    "#classifier.add(Dropout(p=0.1))\n",
    "\n",
    "# Adding the fourth output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'normal', activation = 'linear'))\n",
    "# Compiling Neural Network\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "classifier.summary()\n",
    "history=classifier.fit(X_train, Y_train, batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=1, random_state=0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(X_train, Y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5418886198547216\n"
     ]
    }
   ],
   "source": [
    "Y_pred=clf.predict(X_test)\n",
    "accuracy=accuracy_score(Y_test,Y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5418886198547216\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred=clf.predict(X_test)\n",
    "accuracy=accuracy_score(Y_test,Y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_path</th>\n",
       "      <th>day_zodiac_sign</th>\n",
       "      <th>away_name</th>\n",
       "      <th>home_name</th>\n",
       "      <th>away_lp_day</th>\n",
       "      <th>away_lp</th>\n",
       "      <th>away_zodiac</th>\n",
       "      <th>home_lp_day</th>\n",
       "      <th>home_lp</th>\n",
       "      <th>home_zodiac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41268</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41269</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41270</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41271</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41272</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41273</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41274</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41275</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41276</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41277</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41278</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41279</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41280</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41281</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41282</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41283</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41284</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41285</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41286</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41287</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41288</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41289</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41290</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41291</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41292</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41293</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41294</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41295</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41296</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41297</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41298 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       day_path  day_zodiac_sign  away_name  home_name  away_lp_day  away_lp  \\\n",
       "0             7                1          6         20           11        5   \n",
       "1             8                1         20          6            7        8   \n",
       "2             9                1          9          2            1        9   \n",
       "3             9                1         16          7            6       33   \n",
       "4             9                1         26         11           11        8   \n",
       "5             9                1         14         18            5       11   \n",
       "6             9                1         24         20           11       33   \n",
       "7             9                1          6         27            9        4   \n",
       "8             9                1         19          0           11        1   \n",
       "9             9                1          8          3            6        4   \n",
       "10            9                1         28         17            4       22   \n",
       "11            9                1         10         21            4        9   \n",
       "12            9                1          5         29            4        9   \n",
       "13            9                1         13         30           11        7   \n",
       "14            1                1         22          1           11        5   \n",
       "15            1                1          9          2            9        4   \n",
       "16            1                1         16          7            9        4   \n",
       "17            1                1         26         11            5        5   \n",
       "18            1                1         14         18            3       11   \n",
       "19            1                1         12         23            8        8   \n",
       "20            1                1         19          0            4        3   \n",
       "21            1                1         28         17            4       11   \n",
       "22            1                1         10         21            9        5   \n",
       "23            1                1          4         25            7        8   \n",
       "24            1                1          5         29            3        4   \n",
       "25            1                1         13         30            1        1   \n",
       "26           11                1         22          1            3        5   \n",
       "27           11                1          9          2            1       33   \n",
       "28           11                1         16          7            5       33   \n",
       "29           11                1         26         11            1        3   \n",
       "...         ...              ...        ...        ...          ...      ...   \n",
       "41268        11                4         12          0            1        5   \n",
       "41269        11                4         30          4            1        4   \n",
       "41270        11                4         17          5            7        8   \n",
       "41271        11                4          8         13            8       11   \n",
       "41272        11                4          3         19            4        3   \n",
       "41273        11                4         21         25            1        5   \n",
       "41274        11                4         28         29            9        4   \n",
       "41275        11                4         24          1            3       33   \n",
       "41276        11                4         10          2            5        7   \n",
       "41277        11                4          6          7            7        3   \n",
       "41278        11                4         16          9            8        4   \n",
       "41279        11                4         20         22            6        4   \n",
       "41280        11                4         14         26            1        3   \n",
       "41281        11                4         23         27            1        4   \n",
       "41282        11                4         15         31            3       33   \n",
       "41283         3                4         12          0            8        9   \n",
       "41284         3                4         30          4            1       11   \n",
       "41285         3                4         17          5            9        1   \n",
       "41286         3                4          8         13            1       33   \n",
       "41287         3                4          3         19            6        9   \n",
       "41288         3                4         21         25            1        6   \n",
       "41289         3                4         28         29            5       11   \n",
       "41290         3                4         24          1            5       33   \n",
       "41291         3                4         10          2           11        7   \n",
       "41292         3                4          6          7            7        1   \n",
       "41293         3                4         16          9            4       11   \n",
       "41294         3                4         20         22            8        8   \n",
       "41295         3                4         14         26           11        5   \n",
       "41296         3                4         23         27            4        8   \n",
       "41297         3                4         15         31           11        5   \n",
       "\n",
       "       away_zodiac  home_lp_day  home_lp  home_zodiac  \n",
       "0                0            9        1            7  \n",
       "1                1            5        5            1  \n",
       "2                4            5        4            3  \n",
       "3                2            5        9            3  \n",
       "4                2            4        1            8  \n",
       "5                9            3        7            7  \n",
       "6                6            5        9            9  \n",
       "7                1           11       11            4  \n",
       "8                8            5       11            9  \n",
       "9                5            8        8            4  \n",
       "10               0            9       11            7  \n",
       "11               4            6        5           11  \n",
       "12               6            1        5            1  \n",
       "13               2           11        8            2  \n",
       "14              11            1       11            2  \n",
       "15               4            7        5            3  \n",
       "16               4            4        1            4  \n",
       "17              10            3        5            9  \n",
       "18               5            5        8            8  \n",
       "19               4           11        5            5  \n",
       "20              10            5        4            4  \n",
       "21               2            4        7            2  \n",
       "22               5            5        7            2  \n",
       "23               6            9       11           10  \n",
       "24               9            6        8            0  \n",
       "25               7            5        7            9  \n",
       "26               0           11        1            9  \n",
       "27               4            6        3           10  \n",
       "28               7            8        7           11  \n",
       "29               0            6        4            2  \n",
       "...            ...          ...      ...          ...  \n",
       "41268            2            4        4           11  \n",
       "41269            0            7       33            8  \n",
       "41270            2           11        6            8  \n",
       "41271           11            3        4            6  \n",
       "41272           10           11        9            0  \n",
       "41273            4            3        8            8  \n",
       "41274            3           11        9           11  \n",
       "41275            6            1        3            4  \n",
       "41276           10            8        7            4  \n",
       "41277            7            4        4            2  \n",
       "41278            9            8        4            8  \n",
       "41279            5            3        7            9  \n",
       "41280            1           11       22            3  \n",
       "41281            4            1        1           11  \n",
       "41282            5            5        3           10  \n",
       "41283            3            7        7            1  \n",
       "41284            4            8        3            5  \n",
       "41285            0            3       33            9  \n",
       "41286            7            1        8            7  \n",
       "41287           11            7        5            4  \n",
       "41288            4            8        9           10  \n",
       "41289            9            4        1           11  \n",
       "41290            1            4        1           11  \n",
       "41291            6            9        3           11  \n",
       "41292            9            6        3            8  \n",
       "41293            9            4        3            6  \n",
       "41294            8           11        1            3  \n",
       "41295            1            9        6            9  \n",
       "41296            9            3        3            8  \n",
       "41297           10            9       11            7  \n",
       "\n",
       "[41298 rows x 10 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>day_path</th>\n",
       "      <th>away_lp_day</th>\n",
       "      <th>away_lp</th>\n",
       "      <th>home_lp_day</th>\n",
       "      <th>home_lp</th>\n",
       "      <th>home_win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.060787</td>\n",
       "      <td>-0.019940</td>\n",
       "      <td>0.051028</td>\n",
       "      <td>-0.016576</td>\n",
       "      <td>0.047709</td>\n",
       "      <td>-0.002557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day_path</th>\n",
       "      <td>0.060787</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001636</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>-0.003498</td>\n",
       "      <td>-0.009632</td>\n",
       "      <td>-0.000139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_lp_day</th>\n",
       "      <td>-0.019940</td>\n",
       "      <td>-0.001636</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006197</td>\n",
       "      <td>-0.010272</td>\n",
       "      <td>-0.001877</td>\n",
       "      <td>0.002665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_lp</th>\n",
       "      <td>0.051028</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>-0.006197</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001058</td>\n",
       "      <td>-0.006336</td>\n",
       "      <td>-0.003072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_lp_day</th>\n",
       "      <td>-0.016576</td>\n",
       "      <td>-0.003498</td>\n",
       "      <td>-0.010272</td>\n",
       "      <td>-0.001058</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005660</td>\n",
       "      <td>0.002372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_lp</th>\n",
       "      <td>0.047709</td>\n",
       "      <td>-0.009632</td>\n",
       "      <td>-0.001877</td>\n",
       "      <td>-0.006336</td>\n",
       "      <td>-0.005660</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_win</th>\n",
       "      <td>-0.002557</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>-0.003072</td>\n",
       "      <td>0.002372</td>\n",
       "      <td>-0.009459</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  day_path  away_lp_day   away_lp  home_lp_day   home_lp  \\\n",
       "date         1.000000  0.060787    -0.019940  0.051028    -0.016576  0.047709   \n",
       "day_path     0.060787  1.000000    -0.001636  0.000835    -0.003498 -0.009632   \n",
       "away_lp_day -0.019940 -0.001636     1.000000 -0.006197    -0.010272 -0.001877   \n",
       "away_lp      0.051028  0.000835    -0.006197  1.000000    -0.001058 -0.006336   \n",
       "home_lp_day -0.016576 -0.003498    -0.010272 -0.001058     1.000000 -0.005660   \n",
       "home_lp      0.047709 -0.009632    -0.001877 -0.006336    -0.005660  1.000000   \n",
       "home_win    -0.002557 -0.000139     0.002665 -0.003072     0.002372 -0.009459   \n",
       "\n",
       "             home_win  \n",
       "date        -0.002557  \n",
       "day_path    -0.000139  \n",
       "away_lp_day  0.002665  \n",
       "away_lp     -0.003072  \n",
       "home_lp_day  0.002372  \n",
       "home_lp     -0.009459  \n",
       "home_win     1.000000  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X=X.loc[:,['day_path','away_name','home_name','away_lp_day','away_lp','home_lp_day','home_lp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(new_X,y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5418886198547216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=1, random_state=None,max_features='auto')\n",
    "clf.fit(X_train,Y_train)\n",
    "Y_pred=clf.predict(X_test)\n",
    "score=accuracy_score(Y_pred,Y_test)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5236481033091203\n"
     ]
    }
   ],
   "source": [
    "from sklearn  import tree\n",
    "model=tree.DecisionTreeClassifier(max_depth=10)\n",
    "model.fit(X_train,Y_train)\n",
    "Y_pred=model.predict(X_test)\n",
    "score=accuracy_score(Y_pred,Y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5100080710250202\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model=KNeighborsClassifier(n_neighbors=30)\n",
    "model.fit(X_train,Y_train)\n",
    "Y_pre=model.predict(X_test)\n",
    "acc=accuracy_score(Y_test,Y_pre)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5418886198547216\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "model=svm.SVC(kernel='linear', C=1,gamma=1)\n",
    "model.fit(X_train,Y_train)\n",
    "Y_pred=model.predict(X_test)\n",
    "score=accuracy_score(Y_pred,Y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5412429378531074\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "model=svm.SVC(kernel='rbf', C=10,gamma=1)\n",
    "model.fit(X_train,Y_train)\n",
    "Y_pred=model.predict(X_test)\n",
    "accuracy =accuracy_score(Y_test,Y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5418886198547216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg=LogisticRegression(C=10 , solver='newton-cg')\n",
    "logreg.fit(X_train,Y_train)\n",
    "Y_pred=logreg.predict(X_test)\n",
    "accuracy =accuracy_score(Y_test,Y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5380145278450363\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(X_train, Y_train) \n",
    "Y_pred=clf.predict(X_test)\n",
    "accuracy=accuracy_score(Y_test,Y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5380145278450363\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred=clf.predict(X_test)\n",
    "accuracy=accuracy_score(Y_test,Y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28908, 7)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rosa-mystica/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=7, units=128, kernel_initializer=\"normal\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/home/rosa-mystica/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=256, kernel_initializer=\"normal\")`\n",
      "  \n",
      "/home/rosa-mystica/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=64, kernel_initializer=\"normal\")`\n",
      "  if __name__ == '__main__':\n",
      "/home/rosa-mystica/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"linear\", units=1, kernel_initializer=\"normal\")`\n",
      "  del sys.path[0]\n",
      "/home/rosa-mystica/anaconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_33 (Dense)             (None, 128)               1024      \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 50,561\n",
      "Trainable params: 50,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "28908/28908 [==============================] - 4s 149us/step - loss: 0.7150 - acc: 0.5064\n",
      "Epoch 2/100\n",
      "28908/28908 [==============================] - 4s 135us/step - loss: 0.6935 - acc: 0.5213\n",
      "Epoch 3/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6917 - acc: 0.5314\n",
      "Epoch 4/100\n",
      "28908/28908 [==============================] - 4s 135us/step - loss: 0.6913 - acc: 0.5346\n",
      "Epoch 5/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6912 - acc: 0.5377\n",
      "Epoch 6/100\n",
      "28908/28908 [==============================] - 4s 135us/step - loss: 0.6910 - acc: 0.5382\n",
      "Epoch 7/100\n",
      "28908/28908 [==============================] - 4s 140us/step - loss: 0.6906 - acc: 0.5390\n",
      "Epoch 8/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6905 - acc: 0.5385\n",
      "Epoch 9/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6904 - acc: 0.5379\n",
      "Epoch 10/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6902 - acc: 0.5394\n",
      "Epoch 11/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6903 - acc: 0.5378\n",
      "Epoch 12/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6902 - acc: 0.5382\n",
      "Epoch 13/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6902 - acc: 0.5407\n",
      "Epoch 14/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6900 - acc: 0.5406\n",
      "Epoch 15/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6902 - acc: 0.5408\n",
      "Epoch 16/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6897 - acc: 0.5400\n",
      "Epoch 17/100\n",
      "28908/28908 [==============================] - 4s 141us/step - loss: 0.6898 - acc: 0.5396\n",
      "Epoch 18/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6898 - acc: 0.5395\n",
      "Epoch 19/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6897 - acc: 0.5391\n",
      "Epoch 20/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6896 - acc: 0.5415\n",
      "Epoch 21/100\n",
      "28908/28908 [==============================] - 4s 140us/step - loss: 0.6898 - acc: 0.5420\n",
      "Epoch 22/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6896 - acc: 0.5417\n",
      "Epoch 23/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6895 - acc: 0.5411\n",
      "Epoch 24/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6896 - acc: 0.5417\n",
      "Epoch 25/100\n",
      "28908/28908 [==============================] - 4s 141us/step - loss: 0.6895 - acc: 0.5393\n",
      "Epoch 26/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6895 - acc: 0.5404\n",
      "Epoch 27/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6894 - acc: 0.5419\n",
      "Epoch 28/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6893 - acc: 0.5399\n",
      "Epoch 29/100\n",
      "28908/28908 [==============================] - 4s 146us/step - loss: 0.6891 - acc: 0.5401\n",
      "Epoch 30/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6892 - acc: 0.5406\n",
      "Epoch 31/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6890 - acc: 0.5421\n",
      "Epoch 32/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6891 - acc: 0.5413\n",
      "Epoch 33/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6891 - acc: 0.5415\n",
      "Epoch 34/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6891 - acc: 0.5410\n",
      "Epoch 35/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6888 - acc: 0.5419\n",
      "Epoch 36/100\n",
      "28908/28908 [==============================] - 4s 134us/step - loss: 0.6890 - acc: 0.5421\n",
      "Epoch 37/100\n",
      "28908/28908 [==============================] - 4s 140us/step - loss: 0.6894 - acc: 0.5411\n",
      "Epoch 38/100\n",
      "28908/28908 [==============================] - 4s 149us/step - loss: 0.6888 - acc: 0.5413\n",
      "Epoch 39/100\n",
      "28908/28908 [==============================] - 4s 148us/step - loss: 0.6886 - acc: 0.5428\n",
      "Epoch 40/100\n",
      "28908/28908 [==============================] - 4s 142us/step - loss: 0.6893 - acc: 0.5402\n",
      "Epoch 41/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6909 - acc: 0.5411\n",
      "Epoch 42/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6889 - acc: 0.5403\n",
      "Epoch 43/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6887 - acc: 0.5418\n",
      "Epoch 44/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6909 - acc: 0.5397\n",
      "Epoch 45/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6893 - acc: 0.5418\n",
      "Epoch 46/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6894 - acc: 0.5433\n",
      "Epoch 47/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6888 - acc: 0.5424\n",
      "Epoch 48/100\n",
      "28908/28908 [==============================] - 4s 141us/step - loss: 0.6887 - acc: 0.5422\n",
      "Epoch 49/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6887 - acc: 0.5429\n",
      "Epoch 50/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6891 - acc: 0.5433\n",
      "Epoch 51/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6887 - acc: 0.5437\n",
      "Epoch 52/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6885 - acc: 0.5430\n",
      "Epoch 53/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6897 - acc: 0.5435\n",
      "Epoch 54/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6887 - acc: 0.5426\n",
      "Epoch 55/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6884 - acc: 0.5412\n",
      "Epoch 56/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6887 - acc: 0.5435\n",
      "Epoch 57/100\n",
      "28908/28908 [==============================] - 4s 142us/step - loss: 0.6890 - acc: 0.5421\n",
      "Epoch 58/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6894 - acc: 0.5437\n",
      "Epoch 59/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6882 - acc: 0.5435\n",
      "Epoch 60/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6888 - acc: 0.5434\n",
      "Epoch 61/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6884 - acc: 0.5438\n",
      "Epoch 62/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6883 - acc: 0.5423\n",
      "Epoch 63/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6881 - acc: 0.5436\n",
      "Epoch 64/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6884 - acc: 0.5429\n",
      "Epoch 65/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6881 - acc: 0.5451\n",
      "Epoch 66/100\n",
      "28908/28908 [==============================] - 4s 143us/step - loss: 0.6881 - acc: 0.5436\n",
      "Epoch 67/100\n",
      "28908/28908 [==============================] - 4s 140us/step - loss: 0.6904 - acc: 0.5468\n",
      "Epoch 68/100\n",
      "28908/28908 [==============================] - 4s 140us/step - loss: 0.6884 - acc: 0.5440\n",
      "Epoch 69/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6883 - acc: 0.5443\n",
      "Epoch 70/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6878 - acc: 0.5446\n",
      "Epoch 71/100\n",
      "28908/28908 [==============================] - 4s 140us/step - loss: 0.6877 - acc: 0.5433\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28908/28908 [==============================] - 4s 131us/step - loss: 0.6878 - acc: 0.5448\n",
      "Epoch 73/100\n",
      "28908/28908 [==============================] - 4s 130us/step - loss: 0.6888 - acc: 0.5452\n",
      "Epoch 74/100\n",
      "28908/28908 [==============================] - 4s 131us/step - loss: 0.6891 - acc: 0.5466\n",
      "Epoch 75/100\n",
      "28908/28908 [==============================] - 4s 131us/step - loss: 0.6884 - acc: 0.5453\n",
      "Epoch 76/100\n",
      "28908/28908 [==============================] - 4s 131us/step - loss: 0.6883 - acc: 0.5465\n",
      "Epoch 77/100\n",
      "28908/28908 [==============================] - 4s 132us/step - loss: 0.6873 - acc: 0.5469\n",
      "Epoch 78/100\n",
      "28908/28908 [==============================] - 4s 132us/step - loss: 0.6878 - acc: 0.5467\n",
      "Epoch 79/100\n",
      "28908/28908 [==============================] - 4s 132us/step - loss: 0.6877 - acc: 0.5452\n",
      "Epoch 80/100\n",
      "28908/28908 [==============================] - 4s 143us/step - loss: 0.6886 - acc: 0.5448\n",
      "Epoch 81/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6881 - acc: 0.5433 0s - loss: 0.6882 - acc: 0.54\n",
      "Epoch 82/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6881 - acc: 0.5453\n",
      "Epoch 83/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6883 - acc: 0.5466\n",
      "Epoch 84/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6895 - acc: 0.5448\n",
      "Epoch 85/100\n",
      "28908/28908 [==============================] - 4s 136us/step - loss: 0.6881 - acc: 0.5455\n",
      "Epoch 86/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6875 - acc: 0.5458\n",
      "Epoch 87/100\n",
      "28908/28908 [==============================] - 4s 137us/step - loss: 0.6896 - acc: 0.5461\n",
      "Epoch 88/100\n",
      "28908/28908 [==============================] - 4s 140us/step - loss: 0.6884 - acc: 0.5451\n",
      "Epoch 89/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6872 - acc: 0.5465\n",
      "Epoch 90/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6883 - acc: 0.5460\n",
      "Epoch 91/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6906 - acc: 0.5447\n",
      "Epoch 92/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6881 - acc: 0.5465\n",
      "Epoch 93/100\n",
      "28908/28908 [==============================] - 4s 143us/step - loss: 0.6893 - acc: 0.5461\n",
      "Epoch 94/100\n",
      "28908/28908 [==============================] - 4s 140us/step - loss: 0.6879 - acc: 0.5449\n",
      "Epoch 95/100\n",
      "28908/28908 [==============================] - 4s 141us/step - loss: 0.6886 - acc: 0.5464\n",
      "Epoch 96/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6889 - acc: 0.5457\n",
      "Epoch 97/100\n",
      "28908/28908 [==============================] - 4s 144us/step - loss: 0.6890 - acc: 0.5433\n",
      "Epoch 98/100\n",
      "28908/28908 [==============================] - 4s 139us/step - loss: 0.6879 - acc: 0.5473\n",
      "Epoch 99/100\n",
      "28908/28908 [==============================] - 4s 140us/step - loss: 0.6877 - acc: 0.5458\n",
      "Epoch 100/100\n",
      "28908/28908 [==============================] - 4s 138us/step - loss: 0.6878 - acc: 0.5461\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "classifier=Sequential()\n",
    "classifier.add(Dense(output_dim = 128, init = 'normal', activation = 'relu', input_dim = 7))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 256, init = 'normal', activation = 'relu'))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(output_dim = 64, init = 'normal', activation = 'relu'))\n",
    "#classifier.add(Dropout(p=0.1))\n",
    "\n",
    "# Adding the fourth output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'normal', activation = 'linear'))\n",
    "# Compiling Neural Network\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "classifier.summary()\n",
    "history=classifier.fit(X_train, Y_train, batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3077\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3078\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-451b202b80dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "X[1].values=sc.fit_transform(X[1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
